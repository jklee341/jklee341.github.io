<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>GPU Convolution Optimizations</title>

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Raleway|Poppins" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="../assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="../assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="../assets/vendor/remixicon/remixicon.css" rel="stylesheet">

  <!-- Main CSS File -->
  <link href="../assets/css/style.css" rel="stylesheet">
</head>
<body>

<main id="main">
  <div id="portfolio-details" class="portfolio-details">
    <div class="container">
      <div class="row">
        <div class="col-lg-12 portfolio-info">
          <br>
          <h1>GPU Convolution Optimization with CUDA</h1>
          <h2 style="color:#dae9de">ECE 408: Applied Parallel Programming – Final Project (Individual)</h2>
          <ul>
            <li><strong>Tech Stack</strong>: CUDA, C++, Nsight Systems, Nsight Compute</li>
            <li><strong>Dataset</strong>: Fashion MNIST</li>
          </ul>

          <h2>Overview</h2>
          <p>
            For my ECE 408 final project, I implemented and optimized the forward pass of a convolutional layer using CUDA. The goal was to improve the performance of a neural network layer used in inference for the Fashion MNIST dataset, all while maintaining accuracy and correctness. This project was completed individually and involved multiple GPU-level performance enhancements.
          </p>
          <img src="../assets/img/project/LeNet_Original_Image_48T74Lc.jpg" alt="CUDA Convolution Illustration" class="img-fluid">

          <h2>Optimization Focus</h2>
          <p>
            I experimented with several optimizations including FP16 precision arithmetic, shared and constant memory usage, stream-based concurrency, loop unrolling, and block size tuning. These optimizations were benchmarked using NVIDIA's Nsight Systems and Nsight Compute to assess their impact on kernel performance and memory efficiency.
          </p>

          <h2>Key Outcomes</h2>
          <p>
            The final optimized implementation achieved a combined Op Time of approximately <strong>31ms</strong> for a batch size of 5000—meeting course targets for high-performance inference. Accuracy remained stable at around <strong>0.871</strong>. Some optimizations improved SM throughput and reduced global memory bottlenecks, while others, like streams, introduced tradeoffs in synchronization overhead.
          </p>


          <h2>What I Learned</h2>
          <ol>
            <li><strong>Low-Precision Arithmetic (FP16):</strong> Explored half-precision tradeoffs, including memory savings and conversion overhead.</li>
            <li><strong>Memory Hierarchy Optimization:</strong> Used shared and constant memory to reduce global memory access latency.</li>
            <li><strong>Stream-Based Parallelism:</strong> Learned how to overlap memory transfers and computation using CUDA streams.</li>
            <li><strong>Loop Unrolling & restrict:</strong> Improved inner-loop efficiency and allowed better compiler-level optimization.</li>
            <li><strong>Profiling-Driven Development:</strong> Applied Nsight Systems and Nsight Compute to identify bottlenecks and tune performance.</li>
          </ol>

          <h2>Conclusion</h2>
          <p>
            This project gave me hands-on experience applying GPU programming techniques to a real-world deep learning workload. Beyond coding, I gained a deeper understanding of CUDA performance profiling, memory management, and kernel optimization strategies.
          </p>
        </div>
      </div>
    </div>
  </div>
</main>

<!-- Vendor JS Files -->
<script src="../assets/vendor/jquery/jquery.min.js"></script>
<script src="../assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
<script src="../assets/js/main.js"></script>

</body>
</html>
